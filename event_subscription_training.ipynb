{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e97ee6-86ad-478b-a044-735bec929fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.15.1)\n",
      "Requirement already satisfied: minio in /opt/conda/lib/python3.11/site-packages (7.2.15)\n",
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.11/site-packages (2.9.0)\n",
      "Collecting kserve\n",
      "  Using cached kserve-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2024.8.30)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from minio) (1.26.20)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from minio) (23.1.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.11/site-packages (from minio) (3.23.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.20.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.35.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.18.2)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.4.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: kfp-server-api<2.4.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (2.3.0)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (30.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.11/site-packages (from kfp) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.11/site-packages (from kfp) (0.9.0)\n",
      "Collecting cloudevents<2.0.0,>=1.6.2 (from kserve)\n",
      "  Using cached cloudevents-1.11.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fastapi>=0.115.3 (from kserve)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting grpc-interceptor<1.0.0,>=0.15.4 (from kserve)\n",
      "  Using cached grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.2 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.27.2)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.15 (from kserve)\n",
      "  Using cached orjson-3.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting prometheus-client<0.21.0,>=0.20.0 (from kserve)\n",
      "  Using cached prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting psutil<6.0.0,>=5.9.6 (from kserve)\n",
      "  Using cached psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting pydantic<3,>1.0 (from kserve)\n",
      "  Using cached pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting timing-asgi<0.4.0,>=0.3.0 (from kserve)\n",
      "  Using cached timing_asgi-0.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting uvicorn<0.31.0,>=0.30.6 (from uvicorn[standard]<0.31.0,>=0.30.6->kserve)\n",
      "  Using cached uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Collecting deprecation<3.0,>=2.0 (from cloudevents<2.0.0,>=1.6.2->kserve)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.3->kserve)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.65.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (1.0.6)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.2->kserve) (0.14.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>1.0->kserve)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>1.0->kserve)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>1.0->kserve)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]<0.31.0,>=0.30.6->kserve)\n",
      "  Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]<0.31.0,>=0.30.6->kserve)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]<0.31.0,>=0.30.6->kserve)\n",
      "  Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]<0.31.0,>=0.30.6->kserve)\n",
      "  Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]<0.31.0,>=0.30.6->kserve)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.22)\n",
      "Using cached kserve-0.15.1-py3-none-any.whl (535 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached cloudevents-1.11.1-py3-none-any.whl (55 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
      "Using cached orjson-3.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Using cached prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
      "Using cached psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "Using cached pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached timing_asgi-0.3.1-py3-none-any.whl (5.6 kB)\n",
      "Using cached uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Installing collected packages: websockets, uvloop, uvicorn, typing-inspection, timing-asgi, python-dotenv, pydantic-core, psutil, prometheus-client, orjson, numpy, httptools, grpc-interceptor, deprecation, annotated-types, watchfiles, starlette, pydantic, pandas, cloudevents, fastapi, kserve\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 6.0.0\n",
      "    Uninstalling psutil-6.0.0:\n",
      "      Successfully uninstalled psutil-6.0.0\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus_client 0.21.0\n",
      "    Uninstalling prometheus_client-0.21.0:\n",
      "      Successfully uninstalled prometheus_client-0.21.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "Successfully installed annotated-types-0.7.0 cloudevents-1.11.1 deprecation-2.1.0 fastapi-0.115.12 grpc-interceptor-0.15.4 httptools-0.6.4 kserve-0.15.1 numpy-1.26.4 orjson-3.10.18 pandas-2.2.3 prometheus-client-0.20.0 psutil-5.9.8 pydantic-2.11.5 pydantic-core-2.33.2 python-dotenv-1.1.0 starlette-0.46.2 timing-asgi-0.3.1 typing-inspection-0.4.1 uvicorn-0.30.6 uvloop-0.21.0 watchfiles-1.0.5 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas tensorflow minio kfp kserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359f2848-abad-43c3-b60e-4ac30357618d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/13b479c2-3948-46c3-b3ab-f4748ee44bc7\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/0f938525-abb6-41c3-8a92-2924c3e76f9a\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Metrics, HTML\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=['minio', 'numpy', 'tensorflow']\n",
    ")\n",
    "def kafka_listener() -> NamedTuple('Outputs', [('datapoints_training', float),('datapoints_test', float),('dataset_version', str)]):\n",
    "    \"\"\"\n",
    "    Function to listen to data flow and load it to minio bucket\n",
    "    \"\"\"\n",
    "\n",
    "    from tensorflow import keras\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import json\n",
    "\n",
    "    minio_client = Minio(\n",
    "        \"10.244.0.40:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    # Load MNIST dataset directly from Keras\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # save to numpy file, store in Minio (in your original context, you would store in Minio here)\n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    np.save(\"/tmp/x_test.npy\", x_test)\n",
    "    np.save(\"/tmp/y_test.npy\", y_test)\n",
    "    \n",
    "    try:\n",
    "        minio_client.fput_object(minio_bucket, \"x_train.npy\", \"/tmp/x_train.npy\")\n",
    "        minio_client.fput_object(minio_bucket, \"y_train.npy\", \"/tmp/y_train.npy\")\n",
    "        minio_client.fput_object(minio_bucket, \"x_test.npy\", \"/tmp/x_test.npy\")\n",
    "        minio_client.fput_object(minio_bucket, \"y_test.npy\", \"/tmp/y_test.npy\")\n",
    "    except Exception as e:\n",
    "        print(f\"Datasets already exist: {e}\")\n",
    "        \n",
    "    print(\"listening data\")\n",
    "    minio_client = Minio(\n",
    "        \"10.244.0.40:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    files = [\"x_train.npy\", \"y_train.npy\", \"x_test.npy\", \"y_test.npy\"]\n",
    "    for file in files:\n",
    "        minio_client.fget_object(minio_bucket, file, f\"/tmp/{file}\")\n",
    "\n",
    "    # Cargar datos desde los archivos locales\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "    \n",
    "    dataset_version = \"1.0\"\n",
    "    \n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    outputs_tuple = namedtuple('Outputs', ['datapoints_training', 'datapoints_test', 'dataset_version'])\n",
    "    return outputs_tuple(float(x_train.shape[0]), float(x_test.shape[0]), dataset_version)\n",
    "    \n",
    "    \n",
    "\n",
    "@dsl.component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=['minio', 'numpy']\n",
    ")\n",
    "def reshape_data():\n",
    "    \"\"\"\n",
    "    Reshape the data for model building\n",
    "    \"\"\"\n",
    "    print(\"reshaping data\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "\n",
    "    minio_client = Minio(\n",
    "        \"10.244.0.40:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    # load data from minio\n",
    "    minio_client.fget_object(minio_bucket,\"x_train.npy\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test.npy\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    # reshaping the data\n",
    "    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    \n",
    "    # save data from minio\n",
    "    np.save(\"/tmp/x_train.npy\",x_train)\n",
    "    minio_client.fput_object(minio_bucket,\"x_train.npy\",\"/tmp/x_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_bucket,\"x_test.npy\",\"/tmp/x_test.npy\")\n",
    "\n",
    "@dsl.component(\n",
    "    base_image='tensorflow/tensorflow:2.13.0',\n",
    "    packages_to_install=['minio', 'pandas', 'numpy']\n",
    ")\n",
    "def model_building(\n",
    "    metrics: Output[Metrics],\n",
    "    ui_metadata: Output[HTML],\n",
    "    no_epochs:int = 1,\n",
    "    optimizer: str = \"adam\"):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"10.244.0.40:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax')) #output are 10 classes, numbers from 0-9\n",
    "\n",
    "    #show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    #compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_train.npy\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_train.npy\",\"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    #fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test.npy\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_test.npy\",\"/tmp/y_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "    \n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "\n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "    \n",
    "    metadata_dict = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                  ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "            },\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "\n",
    "'''.format(metric_model_summary,model_accuracy,model_loss),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics_dict = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    ### Save model to minIO\n",
    "    \n",
    "    keras.models.save_model(model,\"/tmp/detect-digits\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import os\n",
    "\n",
    "    minio_client = Minio(\n",
    "            \"10.244.0.40:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "\n",
    "    import glob\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\")\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    upload_local_directory_to_minio(\"/tmp/detect-digits\",minio_bucket,\"models/detect-digits/1/\")\n",
    "    \n",
    "    print(\"Saved model to minIO\")\n",
    "    \n",
    "    with open(metrics.path, \"w\") as f:\n",
    "        json.dump(metrics_dict, f)\n",
    "    \n",
    "    with open(ui_metadata.path, \"w\") as f:\n",
    "        json.dump(metadata_dict, f)\n",
    "\n",
    "\n",
    "\n",
    "@dsl.component(    \n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=['kubernetes', 'kserve']\n",
    ")\n",
    "def model_serving():\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    name='digits-recognizer-{}'.format(v)\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=\"InferenceService\",\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"s3://mlpipeline/models/detect-digits/\"))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='event-susbcription-digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")\n",
    "def output_test(no_epochs:int, optimizer:str):\n",
    "\n",
    "    comp_kafka_listener = kafka_listener()\n",
    "    comp_reshape_data = reshape_data()\n",
    "    comp_model_building = model_building(no_epochs=no_epochs,optimizer=optimizer)\n",
    "    comp_model_serving = model_serving()\n",
    "\n",
    "    step1 = comp_kafka_listener\n",
    "    \n",
    "    step2 = comp_reshape_data\n",
    "    step2.after(step1)\n",
    "    \n",
    "    step3 = comp_model_building\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4 = comp_model_serving\n",
    "    step4.after(step3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "\n",
    "    arguments = {\n",
    "        \"no_epochs\": 1,\n",
    "        \"optimizer\": \"adam\"\n",
    "    }\n",
    "\n",
    "    run_directly = 1\n",
    "    \n",
    "    if (run_directly == 1):\n",
    "        client.create_run_from_pipeline_func(output_test,arguments=arguments,experiment_name=\"event-subscription\",run_name=\"digits-recognizer-event-subscription\")\n",
    "    else:\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=output_test,package_path='output_test_event_subscription.yaml')\n",
    "        client.upload_pipeline_version(pipeline_package_path='output_test_event_subscription.yaml',pipeline_version_name=\"0.4\",pipeline_name='event-susbcription-digits-recognizer-pipeline',description=\"just for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
