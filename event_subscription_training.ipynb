{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f2848-abad-43c3-b60e-4ac30357618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Metrics, HTML, Dataset\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"requests==2.32.3\", \"pandas==2.2.3\"],\n",
    ")\n",
    "def call_model_inference(\n",
    "    model_name: str,\n",
    "    input_data_json: str,\n",
    "    inference_result: Output[Dataset],\n",
    "):\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    try:\n",
    "        parsed_input = json.loads(input_data_json)\n",
    "        input_data = parsed_input.get(\"data\", None)\n",
    "        if input_data is None:\n",
    "            raise ValueError(\"'data' field missing in input_data_json.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid input_data_json: {e}\")\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input-0\",\n",
    "                \"shape\": [len(input_data), len(input_data[0])],\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": input_data,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    model_url = f\"http://{model_name}.kubeflow-user-example-com.svc.cluster.local/v2/models/{model_name}/infer\"\n",
    "    print(\"Sending inference request to:\", model_url)\n",
    "    print(\"Payload:\")\n",
    "    print(json.dumps(payload, indent=2))\n",
    "\n",
    "    response = requests.post(model_url, json=payload)\n",
    "    print(f\"Response status code: {response.status_code}\")\n",
    "    print(f\"Response content: {response.text}\")\n",
    "    response.raise_for_status()\n",
    "\n",
    "    result_json = response.json()\n",
    "    outputs = result_json.get(\"outputs\", [])\n",
    "\n",
    "    if outputs:\n",
    "        output_data = outputs[0].get(\"data\", [])\n",
    "        df = pd.DataFrame(input_data)\n",
    "        df[\"prediction\"] = output_data\n",
    "        df.to_csv(inference_result.path, index=False)\n",
    "        inference_result.metadata[\"source\"] = \"KServe inference\"\n",
    "        inference_result.metadata[\"format\"] = \"csv\"\n",
    "        print(f\"Inference results saved to: {inference_result.path}\")\n",
    "    else:\n",
    "        raise ValueError(\"No 'outputs' found in inference response.\")\n",
    "\n",
    "@dsl.component(base_image=\"python:3.10\")\n",
    "def show_broker_config(\n",
    "    kafka_config: str = \"kafka-headless.message-broker.svc.cluster.local:9092\",\n",
    "    topic: str = \"output_1\",\n",
    "    model_name: str = \"test-model-b\",\n",
    "):\n",
    "    print(\"Kafka broker config:\")\n",
    "    print(f\" - kafka_config: {kafka_config}\")\n",
    "    print(f\" - topic: {topic}\")\n",
    "    print(f\" - model_name: {model_name}\")\n",
    "\n",
    "\n",
    "@dsl.component(base_image=\"python:3.10\", packages_to_install=[\"requests\"])\n",
    "def enqueue_inference(\n",
    "    kafka_config: str = \"kafka-headless.message-broker.svc.cluster.local:9092\",\n",
    "    topic: str = \"output_1\",\n",
    "    model_name: str = \"test-model-b\",\n",
    ") -> str:\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    future_data_point = {\n",
    "        \"co_level\": 17.783347,\n",
    "        \"latitude\": 41.3851,\n",
    "        \"longitude\": 2.1734,\n",
    "        \"hour_of_day\": 1.0,\n",
    "    }\n",
    "\n",
    "    input_data = [\n",
    "        [\n",
    "            future_data_point[\"co_level\"],\n",
    "            future_data_point[\"latitude\"],\n",
    "            future_data_point[\"longitude\"],\n",
    "            future_data_point[\"hour_of_day\"],\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    infer_url = f\"http://{model_name}.kubeflow-user-example-com.svc.cluster.local/v2/models/{model_name}/infer\"\n",
    "\n",
    "    msg = {\n",
    "        \"output_topic\": topic,\n",
    "        \"http_request\": {\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": infer_url,\n",
    "            \"json\": {\n",
    "                \"id\": \"test-id\",\n",
    "                \"parameters\": {\"explain\": True},\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"name\": \"input-0\",\n",
    "                        \"shape\": [len(input_data), 4],\n",
    "                        \"datatype\": \"FP32\",\n",
    "                        \"data\": input_data,\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"Enqueuing inference message:\\n{json.dumps(msg, indent=2)}\")\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://async-bridge.message-broker.svc.cluster.local/api/v1/async-bridge/enqueue/ab-topic\",\n",
    "        json=msg,\n",
    "    )\n",
    "    response_json = response.json()\n",
    "\n",
    "    print(\"Response from enqueue:\", response_json)\n",
    "    result = {\"task_id\": response_json[\"task_id\"], \"sent_data\": msg}\n",
    "\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.10\", packages_to_install=[\"confluent-kafka==2.10.1\"]\n",
    ")\n",
    "def consume_kafka(\n",
    "    task_info: str,\n",
    "    kafka_config: str = \"kafka-headless.message-broker.svc.cluster.local:9092\",\n",
    "    topic: str = \"output_1\",\n",
    ") -> str:\n",
    "    from confluent_kafka import Consumer\n",
    "    import json\n",
    "\n",
    "    conf = {\n",
    "        \"bootstrap.servers\": kafka_config,\n",
    "        \"group.id\": \"my-consumer-group\",\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "    }\n",
    "\n",
    "    print(\"Creating Kafka consumer...\")\n",
    "    consumer = Consumer(conf)\n",
    "    print(\"Kafka consumer created successfully.\")\n",
    "\n",
    "    consumer.subscribe([topic])\n",
    "    print(f\"Listening to topic '{topic}'...\")\n",
    "\n",
    "    payload_result = None\n",
    "\n",
    "    def process_message(message):\n",
    "        nonlocal payload_result\n",
    "        try:\n",
    "            payload = json.loads(message.value().decode(\"utf-8\"))\n",
    "            print(f\"Received payload: {json.dumps(payload, indent=4)}\")\n",
    "            payload_result = payload\n",
    "            consumer.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {e}\")\n",
    "\n",
    "    try:\n",
    "        finish = False\n",
    "        while not finish:\n",
    "            msg = consumer.poll(1.0)\n",
    "            if msg is None:\n",
    "                print(\"No message received, continuing to poll...\")\n",
    "            else:\n",
    "                process_message(msg)\n",
    "                finish = True\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "    return json.dumps(payload_result)\n",
    "\n",
    "    \n",
    "@dsl.component(\n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=['minio', 'numpy', 'keras', 'tensorflow']\n",
    ")\n",
    "def reshape_data():\n",
    "    \"\"\"\n",
    "    Reshape the data for model building\n",
    "    \"\"\"\n",
    "    print(\"reshaping data\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import keras\n",
    "    import tensorflow\n",
    "\n",
    "    minio_client = Minio(\n",
    "        \"10.244.0.40:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "    # Load MNIST dataset directly from Keras\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # save to numpy file, store in Minio (in your original context, you would store in Minio here)\n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    np.save(\"/tmp/x_test.npy\", x_test)\n",
    "    np.save(\"/tmp/y_test.npy\", y_test)\n",
    "    \n",
    "    try:\n",
    "        minio_client.fput_object(minio_bucket, \"x_train.npy\", \"/tmp/x_train.npy\")\n",
    "        minio_client.fput_object(minio_bucket, \"y_train.npy\", \"/tmp/y_train.npy\")\n",
    "        minio_client.fput_object(minio_bucket, \"x_test.npy\", \"/tmp/x_test.npy\")\n",
    "        minio_client.fput_object(minio_bucket, \"y_test.npy\", \"/tmp/y_test.npy\")\n",
    "    except Exception as e:\n",
    "        print(f\"Datasets already exist: {e}\")\n",
    "    \n",
    "    # load data from minio\n",
    "    minio_client.fget_object(minio_bucket,\"x_train.npy\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test.npy\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    # reshaping the data\n",
    "    # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n",
    "    x_train = x_train.reshape(-1,28,28,1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "    # normalizing the data\n",
    "    # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    \n",
    "    # save data from minio\n",
    "    np.save(\"/tmp/x_train.npy\",x_train)\n",
    "    minio_client.fput_object(minio_bucket,\"x_train.npy\",\"/tmp/x_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_bucket,\"x_test.npy\",\"/tmp/x_test.npy\")\n",
    "\n",
    "@dsl.component(\n",
    "    base_image='tensorflow/tensorflow:2.13.0',\n",
    "    packages_to_install=['minio', 'pandas', 'numpy']\n",
    ")\n",
    "def model_building(\n",
    "    metrics: Output[Metrics],\n",
    "    ui_metadata: Output[HTML],\n",
    "    no_epochs:int = 1,\n",
    "    optimizer: str = \"adam\"):\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"10.244.0.40:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax')) #output are 10 classes, numbers from 0-9\n",
    "\n",
    "    #show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    #compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_train.npy\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_train.npy\",\"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    #fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test.npy\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_test.npy\",\"/tmp/y_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "    \n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "\n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "    \n",
    "    metadata_dict = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                  ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "            },\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "\n",
    "'''.format(metric_model_summary,model_accuracy,model_loss),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics_dict = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    ### Save model to minIO\n",
    "    \n",
    "    keras.models.save_model(model,\"/tmp/detect-digits\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import os\n",
    "\n",
    "    minio_client = Minio(\n",
    "            \"10.244.0.40:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "\n",
    "    import glob\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\")\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    upload_local_directory_to_minio(\"/tmp/detect-digits\",minio_bucket,\"models/detect-digits/1/\")\n",
    "    \n",
    "    print(\"Saved model to minIO\")\n",
    "    \n",
    "    with open(metrics.path, \"w\") as f:\n",
    "        json.dump(metrics_dict, f)\n",
    "    \n",
    "    with open(ui_metadata.path, \"w\") as f:\n",
    "        json.dump(metadata_dict, f)\n",
    "\n",
    "\n",
    "\n",
    "@dsl.component(    \n",
    "    base_image='python:3.10',\n",
    "    packages_to_install=['kubernetes', 'kserve']\n",
    ")\n",
    "def model_serving():\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    name='digits-recognizer-event'\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=\"InferenceService\",\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"s3://mlpipeline/models/detect-digits/\"))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='event-susbcription-digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")\n",
    "def event_subscription_pipeline(no_epochs:int, \n",
    "                optimizer:str,\n",
    "                kafka_config: str = \"kafka-headless.message-broker.svc.cluster.local:9092\",\n",
    "                topic: str = \"output_1\",\n",
    "                model_name: str = \"digits-reconizer\",\n",
    "                input_data_json: str = '{\"data\": [[6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6]]}'\n",
    "                ):\n",
    "\n",
    "    comp_config_op = show_broker_config(\n",
    "            kafka_config=kafka_config, topic=topic, model_name=model_name\n",
    "    )\n",
    "\n",
    "    comp_enqueue_op = enqueue_inference(\n",
    "            kafka_config=kafka_config, topic=topic, model_name=model_name\n",
    "    )\n",
    "\n",
    "    comp_consume_kafka = consume_kafka(\n",
    "            task_info=comp_enqueue_op.output, kafka_config=kafka_config, topic=topic\n",
    "    )\n",
    "\n",
    "    comp_reshape_data = reshape_data()\n",
    "    comp_model_building = model_building(no_epochs=no_epochs,optimizer=optimizer)\n",
    "    comp_model_serving = model_serving()\n",
    "\n",
    "    step1 = comp_config_op\n",
    "    \n",
    "    step2 = comp_enqueue_op\n",
    "    step2.after(step1)\n",
    "    \n",
    "    step3 = comp_consume_kafka\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4 = comp_reshape_data\n",
    "    step4.after(step3)\n",
    "    \n",
    "    step5 = comp_model_building\n",
    "    step5.after(step4)\n",
    "    \n",
    "    step6 = comp_model_serving\n",
    "    step6.after(step5)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "\n",
    "    arguments = {\n",
    "        \"no_epochs\": 1,\n",
    "        \"optimizer\": \"adam\"\n",
    "    }\n",
    "\n",
    "    run_directly = 1\n",
    "    \n",
    "    if (run_directly == 1):\n",
    "        client.create_run_from_pipeline_func(event_subscription_pipeline,arguments=arguments,experiment_name=\"event\",run_name=\"digits-recognizer-event-subscription\")\n",
    "    else:\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=event_subscription_pipeline,package_path='event_subscription_pipeline.yaml')\n",
    "        client.upload_pipeline_version(pipeline_package_path='event_subscription_pipeline.yaml',pipeline_version_name=\"0.4\",pipeline_name='event-susbcription-digits-recognizer-pipeline',description=\"just for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
