{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Validation for External/Custom Datasets\n",
    "\n",
    "This notebook demonstrates how to validate external or custom datasets using `ydata-profiling`.\n",
    "\n",
    "Contents:\n",
    "- Requirements and installation\n",
    "- Loading data from various sources (CSV, Excel, JSON, Parquet)\n",
    "- Data preprocessing and transformation\n",
    "- Generating comprehensive data profile reports\n",
    "- Customising the validation report\n",
    "- Comparing datasets\n",
    "- Exporting reports for documentation\n",
    "\n",
    "**Note:** This notebook is designed for datasets that come from external sources and may require preprocessing before validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Python 3.9+\n",
    "- `ydata-profiling` package (dataset profiling)\n",
    "- `pandas` package (data manipulation)\n",
    "- `ipywidgets` package (for interactive widgets)\n",
    "- Optional: `openpyxl` (Excel files), `pyarrow` (Parquet files)\n",
    "\n",
    "Quick installation:\n",
    "```bash\n",
    "pip install ydata-profiling pandas openpyxl pyarrow ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install ydata-profiling pandas openpyxl pyarrow ipywidgets",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Various Sources\n",
    "\n",
    "Choose the appropriate method based on your data source. Uncomment and modify the relevant section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Load from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load from CSV file\n",
    "# Adjust the path and parameters to match your file\n",
    "\n",
    "CSV_PATH = \"./your_dataset.csv\"\n",
    "\n",
    "# Common CSV parameters you may need to adjust:\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    # sep=\";\",           # Delimiter (default is comma)\n",
    "    # encoding=\"utf-8\",  # File encoding\n",
    "    # header=0,          # Row number to use as column names\n",
    "    # skiprows=1,        # Rows to skip at the beginning\n",
    "    # na_values=[\"NA\", \"N/A\", \"\"],  # Additional strings to recognise as NA\n",
    "    # parse_dates=[\"date_column\"],  # Columns to parse as dates\n",
    "    # dtype={\"column_name\": str},   # Specify column data types\n",
    ")\n",
    "\n",
    "print(f\"Data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Load from Excel file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load from Excel file\n",
    "# Uncomment and adjust the path and parameters\n",
    "\n",
    "# EXCEL_PATH = \"./your_dataset.xlsx\"\n",
    "# \n",
    "# df = pd.read_excel(\n",
    "#     EXCEL_PATH,\n",
    "#     sheet_name=0,      # Sheet name or index (0 for first sheet)\n",
    "#     # header=0,        # Row number to use as column names\n",
    "#     # skiprows=1,      # Rows to skip at the beginning\n",
    "#     # usecols=\"A:F\",   # Columns to read (Excel-style range)\n",
    "# )\n",
    "# \n",
    "# print(f\"Data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Load from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load from JSON file\n",
    "# Uncomment and adjust the path and parameters\n",
    "\n",
    "# JSON_PATH = \"./your_dataset.json\"\n",
    "# \n",
    "# # For standard JSON array of objects:\n",
    "# df = pd.read_json(JSON_PATH)\n",
    "# \n",
    "# # For JSON lines format (one JSON object per line):\n",
    "# # df = pd.read_json(JSON_PATH, lines=True)\n",
    "# \n",
    "# # For nested JSON, you may need to normalise:\n",
    "# # import json\n",
    "# # with open(JSON_PATH, 'r') as f:\n",
    "# #     data = json.load(f)\n",
    "# # df = pd.json_normalize(data, record_path=['nested_key'])\n",
    "# \n",
    "# print(f\"Data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4: Load from Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load from Parquet file\n",
    "# Uncomment and adjust the path\n",
    "\n",
    "# PARQUET_PATH = \"./your_dataset.parquet\"\n",
    "# \n",
    "# df = pd.read_parquet(PARQUET_PATH)\n",
    "# \n",
    "# print(f\"Data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 5: Load from URL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load from URL (CSV example)\n",
    "# Uncomment and adjust the URL\n",
    "\n",
    "# DATA_URL = \"https://example.com/your_dataset.csv\"\n",
    "# \n",
    "# df = pd.read_csv(DATA_URL)\n",
    "# \n",
    "# print(f\"Data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using a sample dataset for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Create a sample dataset for demonstration\n",
    "# Remove this cell when using your own data\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"Loading sample dataset for demonstration...\")\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['target'] = housing.target\n",
    "\n",
    "# Add some synthetic issues for demonstration\n",
    "np.random.seed(42)\n",
    "df.loc[np.random.choice(df.index, 100), 'AveRooms'] = np.nan  # Add missing values\n",
    "df.loc[np.random.choice(df.index, 50), 'AveBedrms'] = np.nan\n",
    "df = pd.concat([df, df.iloc[:20]])  # Add duplicates\n",
    "\n",
    "print(f\"Sample data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Preview the data\n",
    "print(\"Data Preview (first 10 rows):\")\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DataFrame info\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "print(\"Basic Statistics:\")\n",
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Summary:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Optional)\n",
    "\n",
    "Apply transformations to prepare your data for profiling. These steps are optional and depend on your data quality requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Data Types"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert columns to appropriate data types\n",
    "# Uncomment and adjust as needed\n",
    "\n",
    "# Convert to datetime\n",
    "# df['date_column'] = pd.to_datetime(df['date_column'], format='%Y-%m-%d')\n",
    "\n",
    "# Convert to numeric\n",
    "# df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')\n",
    "\n",
    "# Convert to categorical\n",
    "# df['category_column'] = df['category_column'].astype('category')\n",
    "\n",
    "# Convert to string\n",
    "# df['string_column'] = df['string_column'].astype(str)\n",
    "\n",
    "print(\"Data types after conversion:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Options for handling missing values (uncomment as needed)\n",
    "\n",
    "# Option 1: Drop rows with any missing values\n",
    "# df_clean = df.dropna()\n",
    "\n",
    "# Option 2: Drop rows with missing values in specific columns\n",
    "# df_clean = df.dropna(subset=['important_column1', 'important_column2'])\n",
    "\n",
    "# Option 3: Fill missing values with a specific value\n",
    "# df['column_name'] = df['column_name'].fillna(0)\n",
    "\n",
    "# Option 4: Fill missing values with mean/median/mode\n",
    "# df['numeric_column'] = df['numeric_column'].fillna(df['numeric_column'].mean())\n",
    "\n",
    "# Option 5: Forward/backward fill\n",
    "# df['column_name'] = df['column_name'].fillna(method='ffill')\n",
    "\n",
    "print(\"Missing values handling: No changes applied (modify as needed)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Uncomment to remove duplicates\n",
    "# df = df.drop_duplicates()\n",
    "# print(f\"After removing duplicates: {len(df)} rows\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Renaming and Selection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Rename columns (uncomment and adjust as needed)\n",
    "# df = df.rename(columns={\n",
    "#     'old_name1': 'new_name1',\n",
    "#     'old_name2': 'new_name2',\n",
    "# })\n",
    "\n",
    "# Select specific columns\n",
    "# df = df[['column1', 'column2', 'column3']]\n",
    "\n",
    "# Drop specific columns\n",
    "# df = df.drop(columns=['unnecessary_column1', 'unnecessary_column2'])\n",
    "\n",
    "print(f\"Current columns: {list(df.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Profile Report\n",
    "\n",
    "Use `ydata-profiling` to generate a comprehensive data quality report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate profile report\n",
    "print(\"Generating data profile report...\")\n",
    "print(\"This may take a few minutes depending on dataset size.\")\n",
    "\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    title=\"External Dataset Validation Report\",\n",
    "    explorative=True,\n",
    ")\n",
    "\n",
    "print(\"Profile report generated successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display the report in the notebook\n# Using to_notebook_iframe() for better compatibility\nprofile.to_notebook_iframe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customising the Report\n",
    "\n",
    "### Minimal Mode (for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Minimal profile for large datasets\nprofile_minimal = ProfileReport(\n    df,\n    title=\"External Dataset - Minimal Report\",\n    minimal=True,  # Faster, less detailed\n)\n\nprofile_minimal.to_notebook_iframe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Custom profile configuration\nprofile_custom = ProfileReport(\n    df,\n    title=\"External Dataset - Custom Report\",\n    # Correlation settings\n    correlations={\n        \"pearson\": {\"calculate\": True},\n        \"spearman\": {\"calculate\": True},\n        \"kendall\": {\"calculate\": False},\n        \"phi_k\": {\"calculate\": False},\n    },\n    # Missing values visualization\n    missing_diagrams={\n        \"bar\": True,\n        \"matrix\": True,\n        \"heatmap\": True,\n    },\n    # Sample size for interactions\n    interactions={\"continuous\": False},\n    # Duplicates detection\n    duplicates={\"head\": 10},\n)\n\nprofile_custom.to_notebook_iframe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Mode\n",
    "\n",
    "If your dataset contains time series data, use the time series mode for specialised analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Time series profile (uncomment if your data is time series)\n# Ensure your DataFrame has a datetime index or column\n\n# # Set datetime column as index\n# df_ts = df.set_index('datetime_column')\n# \n# profile_ts = ProfileReport(\n#     df_ts,\n#     title=\"External Dataset - Time Series Report\",\n#     tsmode=True,\n#     sortby=\"datetime_column\",  # Column to sort by\n# )\n# \n# profile_ts.to_notebook_iframe()\n\nprint(\"Time series mode: Uncomment the cell above if your data is time series.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Two Datasets\n",
    "\n",
    "Compare two versions of a dataset (e.g., before/after cleaning, train/test split)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Example: Compare original vs cleaned dataset\n# Uncomment and adjust as needed\n\n# # Create a \"cleaned\" version for comparison\n# df_cleaned = df.dropna().drop_duplicates()\n# \n# # Generate profiles for both datasets\n# profile_original = ProfileReport(df, title=\"Original Dataset\")\n# profile_cleaned = ProfileReport(df_cleaned, title=\"Cleaned Dataset\")\n# \n# # Generate comparison report\n# comparison_report = profile_original.compare(profile_cleaned)\n# comparison_report.to_notebook_iframe()\n\nprint(\"Dataset comparison: Uncomment the cell above to compare datasets.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Reports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create reports directory\n",
    "REPORTS_DIR = \"./reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for file names\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Export to HTML (self-contained, shareable)\n",
    "html_path = f\"{REPORTS_DIR}/external_dataset_validation_{timestamp}.html\"\n",
    "profile.to_file(html_path)\n",
    "print(f\"HTML report saved to: {html_path}\")\n",
    "\n",
    "# Export to JSON (for programmatic analysis)\n",
    "json_path = f\"{REPORTS_DIR}/external_dataset_validation_{timestamp}.json\"\n",
    "profile.to_file(json_path)\n",
    "print(f\"JSON report saved to: {json_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Report Data Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the profile description as a dictionary\n",
    "description = profile.get_description()\n",
    "\n",
    "# Dataset overview\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"  Number of variables: {description.table['n_var']}\")\n",
    "print(f\"  Number of observations: {description.table['n']}\")\n",
    "print(f\"  Missing cells: {description.table['n_cells_missing']}\")\n",
    "print(f\"  Missing cells (%): {description.table['p_cells_missing']:.2%}\")\n",
    "print(f\"  Duplicate rows: {description.table['n_duplicates']}\")\n",
    "print(f\"  Duplicate rows (%): {description.table['p_duplicates']:.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Variable-level statistics\n",
    "print(\"\\nVariable Types:\")\n",
    "for var_type, count in description.table['types'].items():\n",
    "    print(f\"  {var_type}: {count}\")\n",
    "\n",
    "# Alerts/warnings\n",
    "print(\"\\nAlerts:\")\n",
    "alerts = description.alerts\n",
    "if alerts:\n",
    "    for alert in alerts:\n",
    "        print(f\"  [{alert.alert_type.name}] {alert.column_name}: {alert.alert_type_name}\")\n",
    "else:\n",
    "    print(\"  No alerts found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Checklist\n",
    "\n",
    "Use this checklist to validate your dataset before using it for model training:\n",
    "\n",
    "- [ ] **Completeness**: Missing values are within acceptable limits\n",
    "- [ ] **Uniqueness**: Duplicate records have been addressed\n",
    "- [ ] **Validity**: Data types are correct for each column\n",
    "- [ ] **Accuracy**: Values are within expected ranges\n",
    "- [ ] **Consistency**: Related columns have consistent values\n",
    "- [ ] **Timeliness**: Data is current and relevant"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Automated data quality checks\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Completeness\n",
    "missing_pct = description.table['p_cells_missing'] * 100\n",
    "completeness_status = \"PASS\" if missing_pct < 5 else \"WARN\" if missing_pct < 20 else \"FAIL\"\n",
    "print(f\"\\nCompleteness: {completeness_status}\")\n",
    "print(f\"  Missing cells: {missing_pct:.2f}%\")\n",
    "\n",
    "# Uniqueness\n",
    "duplicate_pct = description.table['p_duplicates'] * 100\n",
    "uniqueness_status = \"PASS\" if duplicate_pct < 1 else \"WARN\" if duplicate_pct < 5 else \"FAIL\"\n",
    "print(f\"\\nUniqueness: {uniqueness_status}\")\n",
    "print(f\"  Duplicate rows: {duplicate_pct:.2f}%\")\n",
    "\n",
    "# Alerts summary\n",
    "alert_count = len(description.alerts)\n",
    "alerts_status = \"PASS\" if alert_count == 0 else \"WARN\" if alert_count < 5 else \"FAIL\"\n",
    "print(f\"\\nAlerts: {alerts_status}\")\n",
    "print(f\"  Total alerts: {alert_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tips and Troubleshooting\n\n- **Large datasets**: Use `minimal=True` or sample your data:\n  ```python\n  df_sample = df.sample(n=10000, random_state=42)\n  profile = ProfileReport(df_sample, minimal=True)\n  ```\n\n- **Memory issues**: Reduce dataset size or disable heavy computations:\n  ```python\n  profile = ProfileReport(df, interactions={\"continuous\": False}, correlations=None)\n  ```\n\n- **Encoding issues with CSV**: Try different encodings:\n  ```python\n  df = pd.read_csv(path, encoding='latin-1')  # or 'cp1252', 'iso-8859-1'\n  ```\n\n- **Date parsing issues**: Specify the format explicitly:\n  ```python\n  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n  ```\n\n- **High cardinality categorical columns**: Limit unique values shown:\n  ```python\n  profile = ProfileReport(df, vars={\"cat\": {\"n_obs\": 5}})\n  ```\n\n- **Widget rendering issues**: If `to_widgets()` fails with compatibility errors, use `to_notebook_iframe()` instead for reliable HTML rendering in the notebook."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
