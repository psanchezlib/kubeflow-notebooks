{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Validation for External/Custom Datasets\n",
    "\n",
    "This notebook demonstrates how to validate external or custom datasets using `ydata-profiling`.\n",
    "\n",
    "Contents:\n",
    "- Requirements and installation\n",
    "- Loading data from various sources (CSV, Excel, JSON, Parquet)\n",
    "- Data preprocessing and transformation\n",
    "- Generating comprehensive data profile reports\n",
    "- Customising the validation report\n",
    "- Comparing datasets\n",
    "- Exporting reports for documentation\n",
    "\n",
    "**Note:** This notebook is designed for datasets that come from external sources and may require preprocessing before validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Python 3.9+\n",
    "- `ydata-profiling` package (dataset profiling)\n",
    "- `pandas` package (data manipulation)\n",
    "- `ipywidgets` package (for interactive widgets)\n",
    "- Optional: `openpyxl` (Excel files), `pyarrow` (Parquet files)\n",
    "\n",
    "Quick installation:\n",
    "```bash\n",
    "pip install ydata-profiling pandas openpyxl pyarrow ipywidgets scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install ydata-profiling pandas openpyxl pyarrow ipywidgets scikit-learn",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Various Sources\n",
    "\n",
    "Choose the appropriate method based on your data source. Uncomment and modify the relevant section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using a sample dataset for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Example: Create a sample dataset for demonstration\n# Remove this cell when using your own data\n\nfrom sklearn.datasets import fetch_california_housing\n\nprint(\"Loading sample dataset for demonstration...\")\nhousing = fetch_california_housing()\ndf = pd.DataFrame(housing.data, columns=housing.feature_names)\ndf['target'] = housing.target\n\n# Add some synthetic issues for demonstration\nrng = np.random.default_rng(42)\ndf.loc[rng.choice(df.index, 100, replace=False), 'AveRooms'] = np.nan  # Add missing values\ndf.loc[rng.choice(df.index, 50, replace=False), 'AveBedrms'] = np.nan\ndf = pd.concat([df, df.iloc[:20]])  # Add duplicates\n\nprint(f\"Sample data loaded: {len(df)} rows, {len(df.columns)} columns\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Preview the data\n",
    "print(\"Data Preview (first 10 rows):\")\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# DataFrame info\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "print(\"Basic Statistics:\")\n",
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Summary:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Optional)\n",
    "\n",
    "Apply transformations to prepare your data for profiling. These steps are optional and depend on your data quality requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Data Types"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert columns to appropriate data types\n",
    "# Uncomment and adjust as needed\n",
    "\n",
    "# Convert to datetime\n",
    "# df['date_column'] = pd.to_datetime(df['date_column'], format='%Y-%m-%d')\n",
    "\n",
    "# Convert to numeric\n",
    "# df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')\n",
    "\n",
    "# Convert to categorical\n",
    "# df['category_column'] = df['category_column'].astype('category')\n",
    "\n",
    "# Convert to string\n",
    "# df['string_column'] = df['string_column'].astype(str)\n",
    "\n",
    "print(\"Data types after conversion:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Options for handling missing values (uncomment as needed)\n",
    "\n",
    "# Option 1: Drop rows with any missing values\n",
    "# df_clean = df.dropna()\n",
    "\n",
    "# Option 2: Drop rows with missing values in specific columns\n",
    "# df_clean = df.dropna(subset=['important_column1', 'important_column2'])\n",
    "\n",
    "# Option 3: Fill missing values with a specific value\n",
    "# df['column_name'] = df['column_name'].fillna(0)\n",
    "\n",
    "# Option 4: Fill missing values with mean/median/mode\n",
    "# df['numeric_column'] = df['numeric_column'].fillna(df['numeric_column'].mean())\n",
    "\n",
    "# Option 5: Forward/backward fill\n",
    "# df['column_name'] = df['column_name'].fillna(method='ffill')\n",
    "\n",
    "print(\"Missing values handling: No changes applied (modify as needed)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Uncomment to remove duplicates\n",
    "# df = df.drop_duplicates()\n",
    "# print(f\"After removing duplicates: {len(df)} rows\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Renaming and Selection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Rename columns (uncomment and adjust as needed)\n",
    "# df = df.rename(columns={\n",
    "#     'old_name1': 'new_name1',\n",
    "#     'old_name2': 'new_name2',\n",
    "# })\n",
    "\n",
    "# Select specific columns\n",
    "# df = df[['column1', 'column2', 'column3']]\n",
    "\n",
    "# Drop specific columns\n",
    "# df = df.drop(columns=['unnecessary_column1', 'unnecessary_column2'])\n",
    "\n",
    "print(f\"Current columns: {list(df.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Profile Report\n",
    "\n",
    "Use `ydata-profiling` to generate a comprehensive data quality report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate profile report\n",
    "print(\"Generating data profile report...\")\n",
    "print(\"This may take a few minutes depending on dataset size.\")\n",
    "\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    title=\"External Dataset Validation Report\",\n",
    "    explorative=True,\n",
    ")\n",
    "\n",
    "print(\"Profile report generated successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display the report in the notebook\n# Using to_notebook_iframe() for better compatibility\nprofile.to_notebook_iframe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customising the Report\n",
    "\n",
    "### Minimal Mode (for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Minimal profile for large datasets\nprofile_minimal = ProfileReport(\n    df,\n    title=\"External Dataset - Minimal Report\",\n    minimal=True,  # Faster, less detailed\n)\n\nprofile_minimal.to_notebook_iframe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Custom profile configuration\nprofile_custom = ProfileReport(\n    df,\n    title=\"External Dataset - Custom Report\",\n    # Correlation settings\n    correlations={\n        \"pearson\": {\"calculate\": True},\n        \"spearman\": {\"calculate\": True},\n        \"kendall\": {\"calculate\": False},\n        \"phi_k\": {\"calculate\": False},\n    },\n    # Missing values visualization\n    missing_diagrams={\n        \"bar\": True,\n        \"matrix\": True,\n        \"heatmap\": True,\n    },\n    # Sample size for interactions\n    interactions={\"continuous\": False},\n    # Duplicates detection\n    duplicates={\"head\": 10},\n)\n\nprofile_custom.to_notebook_iframe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Time Series Mode\n\nIf your dataset contains time series data, use the time series mode for specialised analysis.\n\nTo enable time series profiling:\n1. Set your datetime column as the DataFrame index: `df_ts = df.set_index('datetime_column')`\n2. Create the profile with `tsmode=True` and `sortby` parameter:\n   ```python\n   profile_ts = ProfileReport(\n       df_ts,\n       title=\"External Dataset - Time Series Report\",\n       tsmode=True,\n       sortby=\"datetime_column\",\n   )\n   profile_ts.to_notebook_iframe()\n   ```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparing Two Datasets\n\nCompare two versions of a dataset (e.g., before/after cleaning, train/test split).\n\nTo compare datasets:\n1. Create profiles for both datasets\n2. Use the `compare()` method to generate a comparison report\n\n```python\ndf_cleaned = df.dropna().drop_duplicates()\n\nprofile_original = ProfileReport(df, title=\"Original Dataset\")\nprofile_cleaned = ProfileReport(df_cleaned, title=\"Cleaned Dataset\")\n\ncomparison_report = profile_original.compare(profile_cleaned)\ncomparison_report.to_notebook_iframe()\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Report Data Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the profile description as a dictionary\n",
    "description = profile.get_description()\n",
    "\n",
    "# Dataset overview\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"  Number of variables: {description.table['n_var']}\")\n",
    "print(f\"  Number of observations: {description.table['n']}\")\n",
    "print(f\"  Missing cells: {description.table['n_cells_missing']}\")\n",
    "print(f\"  Missing cells (%): {description.table['p_cells_missing']:.2%}\")\n",
    "print(f\"  Duplicate rows: {description.table['n_duplicates']}\")\n",
    "print(f\"  Duplicate rows (%): {description.table['p_duplicates']:.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Variable-level statistics\n",
    "print(\"\\nVariable Types:\")\n",
    "for var_type, count in description.table['types'].items():\n",
    "    print(f\"  {var_type}: {count}\")\n",
    "\n",
    "# Alerts/warnings\n",
    "print(\"\\nAlerts:\")\n",
    "alerts = description.alerts\n",
    "if alerts:\n",
    "    for alert in alerts:\n",
    "        print(f\"  [{alert.alert_type.name}] {alert.column_name}: {alert.alert_type_name}\")\n",
    "else:\n",
    "    print(\"  No alerts found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Checklist\n",
    "\n",
    "Use this checklist to validate your dataset before using it for model training:\n",
    "\n",
    "- [ ] **Completeness**: Missing values are within acceptable limits\n",
    "- [ ] **Uniqueness**: Duplicate records have been addressed\n",
    "- [ ] **Validity**: Data types are correct for each column\n",
    "- [ ] **Accuracy**: Values are within expected ranges\n",
    "- [ ] **Consistency**: Related columns have consistent values\n",
    "- [ ] **Timeliness**: Data is current and relevant"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Automated data quality checks\nprint(\"=\" * 60)\nprint(\"DATA QUALITY SUMMARY\")\nprint(\"=\" * 60)\n\n\ndef get_quality_status(value, pass_threshold, warn_threshold):\n    \"\"\"Determine quality status based on thresholds.\"\"\"\n    if value < pass_threshold:\n        return \"PASS\"\n    if value < warn_threshold:\n        return \"WARN\"\n    return \"FAIL\"\n\n\ndef get_alert_status(alert_count, pass_threshold, warn_threshold):\n    \"\"\"Determine alert status based on count thresholds.\"\"\"\n    if alert_count <= pass_threshold:\n        return \"PASS\"\n    if alert_count < warn_threshold:\n        return \"WARN\"\n    return \"FAIL\"\n\n\n# Completeness\nmissing_pct = description.table['p_cells_missing'] * 100\ncompleteness_status = get_quality_status(missing_pct, pass_threshold=5, warn_threshold=20)\nprint(f\"\\nCompleteness: {completeness_status}\")\nprint(f\"  Missing cells: {missing_pct:.2f}%\")\n\n# Uniqueness\nduplicate_pct = description.table['p_duplicates'] * 100\nuniqueness_status = get_quality_status(duplicate_pct, pass_threshold=1, warn_threshold=5)\nprint(f\"\\nUniqueness: {uniqueness_status}\")\nprint(f\"  Duplicate rows: {duplicate_pct:.2f}%\")\n\n# Alerts summary\ntotal_alerts = len(description.alerts)\nalerts_status = get_alert_status(total_alerts, pass_threshold=0, warn_threshold=5)\nprint(f\"\\nAlerts: {alerts_status}\")\nprint(f\"  Total alerts: {total_alerts}\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Tips and Troubleshooting\n\n- **Large datasets**: Use `minimal=True` or sample your data:\n  ```python\n  df_sample = df.sample(n=10000, random_state=42)\n  profile = ProfileReport(df_sample, minimal=True)\n  ```\n\n- **Memory issues**: Reduce dataset size or disable heavy computations:\n  ```python\n  profile = ProfileReport(df, interactions={\"continuous\": False}, correlations=None)\n  ```\n\n- **Encoding issues with CSV**: Try different encodings:\n  ```python\n  df = pd.read_csv(path, encoding='latin-1')  # or 'cp1252', 'iso-8859-1'\n  ```\n\n- **Date parsing issues**: Specify the format explicitly:\n  ```python\n  df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n  ```\n\n- **High cardinality categorical columns**: Limit unique values shown:\n  ```python\n  profile = ProfileReport(df, vars={\"cat\": {\"n_obs\": 5}})\n  ```\n\n- **Widget rendering issues**: If `to_widgets()` fails with compatibility errors, use `to_notebook_iframe()` instead for reliable HTML rendering in the notebook."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
